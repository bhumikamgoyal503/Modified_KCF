# -*- coding: utf-8 -*-
"""RC_intern_Goyal_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fm-h3XR4A2Dwv3Om-CkfquYeYYJvU0bR
"""

import pandas as pd
import numpy as np
from scipy.stats import skew, kurtosis, norm, lognorm, gamma, kstest

#Load the data
xls = pd.ExcelFile("data_income_consumption_gender.xlsx")
#Excel file contains only one sheet of the name Data Task_Intern, we need not to apply sheet indexing.
data = pd.read_excel("data_income_consumption_gender.xlsx",header=None, names=["Income", "Consumption", "Gender"])

#Now check the first few rows to confirm the data structure
print(data.head())# head is used to print first few rows.

#Data inspection and cleaning
print("\nData Information:\n")
print(data.info())

#Since the data has 961 entries and there are no missing values, we can proceed without additional cleaning steps.

#Calculate Summary Statistics
summary_stats = pd.DataFrame()

# Metrics to calculate
metrics = ["Count", "Mean", "Standard Deviation", "Minimum", "25th Percentile", "Median", "75th Percentile", "Maximum", "Skewness", "Kurtosis"]

# Calculating statistics for each column
for col in data.columns:
    count = data[col].count()
    mean = data[col].mean()
    std = data[col].std()
    minimum = data[col].min()
    q1 = data[col].quantile(0.25)
    median = data[col].median()
    q3 = data[col].quantile(0.75)
    maximum = data[col].max()
    skewness = skew(data[col])
    kurt = kurtosis(data[col])

# Create a list of values for each metric
    values = [count, mean, std, minimum, q1, median, q3, maximum, skewness, kurt]

# Adding the values to the summary table
    summary_stats[col] = values

# Assigning metrics as the index
    summary_stats.index = metrics

#Summary Table
print("\nSummary Statistics Table:\n")
print(summary_stats)

import matplotlib.pyplot as plt
import seaborn as sns


#Plot the Histogram of income
plt.figure(figsize=(10, 6))
sns.histplot(data["Income"], kde=True, stat="density", color='blue', label='Empirical Data')
plt.title('Histogram of Income with KDE')
plt.xlabel('Income')
plt.ylabel('Density')
plt.show()
#Normalizing the histogram allows us to treat it as a probability distribution

from scipy.stats import lognorm, gamma

#Fit Lognormal Distribution using MLE
shape, loc, scale = lognorm.fit(data["Income"])
lognorm_dist = lognorm(s=shape, loc=loc, scale=scale)
lognorm_pdf = lognorm_dist.pdf(np.linspace(data["Income"].min(), data["Income"].max(), 100))

#Fit Gamma Distribution using MLE
alpha, loc_gamma, scale_gamma = gamma.fit(data["Income"])
gamma_dist = gamma(a=alpha, loc=loc_gamma, scale=scale_gamma)
gamma_pdf = gamma_dist.pdf(np.linspace(data["Income"].min(), data["Income"].max(), 100))

#Plot both fitted distributions
plt.figure(figsize=(10, 6))
sns.histplot(data["Income"], kde=True, stat="density", color='blue', label='Empirical Data')
plt.plot(np.linspace(data["Income"].min(), data["Income"].max(), 100), lognorm_pdf, label="Lognormal", color='red')
plt.plot(np.linspace(data["Income"].min(), data["Income"].max(), 100), gamma_pdf, label="Gamma", color='green')
plt.title("Lognormal vs Gamma Distribution Fit")
plt.xlabel("Income")
plt.ylabel("Density")
plt.legend()
plt.show()

from scipy.stats import kstest

#Goodness of fit metric akaike information criterion (AIC)
lognorm_ll = np.sum(np.log(lognorm_dist.pdf(data["Income"])))
gamma_ll = np.sum(np.log(gamma_dist.pdf(data["Income"])))

lognorm_aic = 2 * 3 - 2 * lognorm_ll  # 3 parameters: shape, loc, scale
gamma_aic = 2 * 3 - 2 * gamma_ll      # 3 parameters: alpha, loc, scale

print(f"Lognormal AIC: {lognorm_aic}")
print(f"Gamma AIC: {gamma_aic}")



#Goodness of fit metric kolmogorov-smirnov (KS)
ks_lognorm = kstest(data["Income"], "lognorm", args=(shape, loc, scale))
ks_gamma = kstest(data["Income"], "gamma", args=(alpha, loc_gamma, scale_gamma))

print("\nKS Test - Lognormal:", ks_lognorm)
print("KS Test - Gamma:", ks_gamma)

#Determine the better fit based on AIC and KS Test
if lognorm_aic < gamma_aic and ks_lognorm.pvalue > ks_gamma.pvalue:
    best_fit = "lognorm"
    print("\nSelected Distribution: Lognormal")
else:
    best_fit = "gamma"
    print("\nSelected Distribution: Gamma")

import statsmodels.api as sm

#Applying Log tranformation based on lognormal Distribution
data["Log_Income"] = np.log(data["Income"])

#PREPARING DATASET FOR OLS REGRESSION

#Independent variables: Log_Income and Gender
X = data[["Log_Income", "Gender"]]
X= sm.add_constant(X) #Adding constant for intercept

#Dependent variables: Consumption
y = data["Consumption"]

#Run OLS
model = sm.OLS(y, X).fit()

#Extract the coefficient for log_income
beta_1 = model.params["Log_Income"]
print(f"Coefficient for Log_Income (β1): {beta_1}")

#Calculating average consumption and average income
avg_consumption = data["Consumption"].mean()
avg_income = data["Income"].mean()

#Corrected MPC
corrected_mpc = beta_1 / avg_income

print(f"Corrected MPC: {corrected_mpc}")
print(f"Interpretation: On average, a $1 increase in income leads to a ${corrected_mpc:.2f} increase in consumption.")



# Extract coefficients
beta_0 = model.params["const"]
beta_1 = model.params["Log_Income"]
beta_2 = model.params["Gender"]


# Print the general model equation
print("\n--- General Model Equation ---")
print("Predicted Consumption (Ĉ) = β₀ + β₁ * log(Income) + β₂ * Gender")

# Print the equation with actual estimated values
print("\n--- Estimated Model Equation ---")
print(f"Ĉ = {beta_0:.2f} + {beta_1:.2f} * log(Income) + {beta_2:.2f} * Gender")

# Print the corrected MPC
print("\n--- Corrected MPC ---")
print(f"Corrected MPC (∂C/∂Y) = β₁ / Y = {beta_1:.2f} / {avg_income:.2f} = {corrected_mpc:.4f}")